{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn # it is stand for Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `nn.Linear`\n",
        "\n",
        "To create a linear layer, you need to pass it the number of input dimensions and the number of output dimensions. The linear object initialized as `nn.Linear(10, 2)` will take in a $n\\times10$ matrix and return an $n\\times2$ matrix, where all $n$ elements have had the same linear transformation performed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.5473,  0.8653],\n",
              "        [ 0.2047,  0.1283],\n",
              "        [-1.6952,  0.0803]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear = nn.Linear(10, 2)\n",
        "example_input = torch.randn(3, 10)\n",
        "example_output = linear(example_input)\n",
        "example_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `nn.ReLU`\n",
        "\n",
        "`nn.ReLU()` will create an object that, when receiving a tensor, will perform a ReLU activation function. A ReLU non-linearity sets all negative numbers in a tensor to zero. In general, the simplest neural networks are composed of series of linear transformations, each followed by activation functions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.8653],\n",
              "        [0.2047, 0.1283],\n",
              "        [0.0000, 0.0803]], grad_fn=<ReluBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relu = nn.ReLU()\n",
        "relu_output = relu(example_output)\n",
        "relu_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `nn.BatchNorm1d`\n",
        "\n",
        "`nn.BatchNorm1d` is a normalization technique that will rescale a batch of $n$ inputs to have a consistent mean and standard deviation between batches. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7067,  1.4121],\n",
              "        [ 1.4135, -0.6392],\n",
              "        [-0.7067, -0.7728]], grad_fn=<NativeBatchNormBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchnorm = nn.BatchNorm1d(2)\n",
        "batchnorm_output = batchnorm(relu_output)\n",
        "batchnorm_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `nn.Sequential`\n",
        "\n",
        "`nn.Sequential` creates a single operation that performs a sequence of operations. For example, you can write a neural network layer with a batch normalization as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: \n",
            "tensor([[ 2.5846e+00,  4.9193e-01,  5.8913e-01,  3.0832e+00, -2.0777e-01],\n",
            "        [ 3.9364e-01,  8.0023e-01,  2.2960e+00,  9.3966e-01,  4.4342e-01],\n",
            "        [ 1.6927e+00,  2.3627e+00,  1.9047e-01,  6.2804e-01,  3.3643e-01],\n",
            "        [ 2.0554e+00,  1.2780e+00,  9.9691e-01,  8.5513e-01, -9.9841e-02],\n",
            "        [ 4.9251e-04,  6.6356e-01,  1.0199e+00, -1.9589e-01,  1.8946e+00]])\n",
            "output: \n",
            "tensor([[0.2432, 1.8097],\n",
            "        [0.9818, 0.0000],\n",
            "        [0.0000, 0.0000],\n",
            "        [0.0000, 0.1172],\n",
            "        [1.0129, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mlp_layer = nn.Sequential(\n",
        "    nn.Linear(5, 2),\n",
        "    nn.BatchNorm1d(2),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "test_example = torch.randn(5,5) + 1\n",
        "print(\"input: \")\n",
        "print(test_example)\n",
        "print(\"output: \")\n",
        "print(mlp_layer(test_example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizers\n",
        "\n",
        "To create an optimizer in PyTorch, you'll need to use the `torch.optim` module, often imported as `optim`.   \n",
        "`optim.Adam` corresponds to the Adam optimizer. To create an optimizer object, you'll need to pass it the parameters to be optimized and the learning rate, `lr`, as well as any other parameters specific to the optimizer.\n",
        "\n",
        "For all `nn` objects, you can access their parameters as a list using their `parameters()` method, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "A (basic) training step in PyTorch consists of four basic parts:\n",
        "\n",
        "\n",
        "1.   Set all of the gradients to zero using `opt.zero_grad()`\n",
        "2.   Calculate the loss, `loss`\n",
        "3.   Calculate the gradients with respect to the loss using `loss.backward()`\n",
        "4.   Update the parameters being optimized using `opt.step()`\n",
        "\n",
        "That might look like the following code (and you'll notice that if you run it several times, the loss goes down):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7543, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "train_example = torch.randn(100,5) + 1\n",
        "adam_opt.zero_grad()\n",
        "\n",
        "# We'll use a simple loss function of mean distance from 1\n",
        "# torch.abs takes the absolute value of a tensor\n",
        "cur_loss = torch.abs(1 - mlp_layer(train_example)).mean()\n",
        "\n",
        "cur_loss.backward()\n",
        "adam_opt.step()\n",
        "print(cur_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New `nn` Classes\n",
        "\n",
        "You can also create new classes which extend the `nn` module. For these classes, all class attributes, as in `self.layer` or `self.param` will automatically treated as parameters if they are themselves `nn` objects or if they are tensors wrapped in `nn.Parameter` which are initialized with the class. \n",
        "\n",
        "The `__init__` function defines what will happen when the object is created. The first line of the init function of a class, for example, `WellNamedClass`, needs to be `super(WellNamedClass, self).__init__()`. \n",
        "\n",
        "The `forward` function defines what runs if you create that object `model` and pass it a tensor `x`, as in `model(x)`. If you choose the function signature, `(self, x)`, then each call of the forward function, gets two pieces of information: `self`, which is a reference to the object with which you can access all of its parameters, and `x`, which is the current tensor for which you'd like to return `y`.\n",
        "\n",
        "One class might look like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExampleModule(nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super(ExampleModule, self).__init__()\n",
        "        self.linear = nn.Linear(input_dims, output_dims)\n",
        "        self.exponent = nn.Parameter(torch.tensor(1.))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "\n",
        "        # This is the notation for element-wise exponentiation, \n",
        "        # which matches python in general\n",
        "        x = x ** self.exponent \n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor(1., requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.0128, -0.1707, -0.0394,  0.2377,  0.2220, -0.0957,  0.0123,  0.2168,\n",
              "           0.3161, -0.0836],\n",
              "         [-0.2541,  0.2870,  0.0894,  0.1800, -0.2800, -0.1936, -0.0727, -0.3009,\n",
              "          -0.0031,  0.0273]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.2845,  0.3094], requires_grad=True)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_model = ExampleModule(10, 2)\n",
        "list(example_model.parameters())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
